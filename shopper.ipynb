{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e95b57",
   "metadata": {},
   "source": [
    "Project Name: ðŸ›’ Shopper Spectrum: Customer Segmentation and Product Recommendations in E-Commerce \n",
    "\n",
    "Project Type: Unsupervised Machine Learning â€“ Clustering & Collaborative Filtering â€“ Recommendation System \n",
    "\n",
    "Member Name: Aryan Dubey\n",
    "\n",
    "This project analyzes transaction data from an online retail business to uncover patterns in customer purchasing behavior. The goal is to segment customers based on Recency, Frequency, and Monetary (RFM) analysis and develop a product recommendation system using collaborative filtering techniques. The project uses various data science methods and technologies, including data cleaning, feature engineering, and exploratory data analysis (EDA). We utilize K-Means clustering for customer segmentation and cosine similarity for the recommendation system. The final deliverables include a Python notebook and a Streamlit web application that provides real-time outputs for both customer segmentation and product recommendations.\n",
    "Project Name: ðŸ›’ Shopper Spectrum: Customer Segmentation and Product Recommendations in E-Commerce \n",
    "\n",
    "Project Type: Unsupervised Machine Learning â€“ Clustering & Collaborative Filtering â€“ Recommendation System \n",
    "\n",
    "This project analyzes transaction data from an online retail business to uncover patterns in customer purchasing behavior. The goal is to segment customers based on Recency, Frequency, and Monetary (RFM) analysis and develop a product recommendation system using collaborative filtering techniques. The project uses various data science methods and technologies, including data cleaning, feature engineering, and exploratory data analysis (EDA). We utilize K-Means clustering for customer segmentation and cosine similarity for the recommendation system. The final deliverables include a Python notebook and a Streamlit web application that provides real-time outputs for both customer segmentation and product recommendations.\n",
    "\n",
    "Key Project Elements:\n",
    "\n",
    "Domain: E-Commerce and Retail Analytics \n",
    "\n",
    "Skills & Methods: Public Dataset Exploration and Preprocessing, Data Cleaning and Feature Engineering, Exploratory Data Analysis (EDA), Clustering Techniques, Collaborative Filtering-based Product Recommendation, and Model Evaluation.\n",
    "\n",
    "Technologies & Libraries:\n",
    "\n",
    "Data Manipulation: Pandas, Numpy \n",
    "\n",
    "Machine Learning: Scikit-Learn (K-Means Clustering, StandardScaler, Cosine Similarity) \n",
    "\n",
    "Visualization: DataVisualization \n",
    "\n",
    "Web Application: Streamlit \n",
    "\n",
    "Problem Type: Unsupervised Machine Learning (Clustering) and Collaborative Filtering (Recommendation System).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c7bd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Preview:\n",
      "  InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1    536365     71053                  WHITE METAL LANTERN         6   \n",
      "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "           InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0  2022-12-01 08:26:00       2.55     17850.0  United Kingdom  \n",
      "1  2022-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
      "2  2022-12-01 08:26:00       2.75     17850.0  United Kingdom  \n",
      "3  2022-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
      "4  2022-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
      "\n",
      "Data Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   InvoiceNo    541909 non-null  object \n",
      " 1   StockCode    541909 non-null  object \n",
      " 2   Description  540455 non-null  object \n",
      " 3   Quantity     541909 non-null  int64  \n",
      " 4   InvoiceDate  541909 non-null  object \n",
      " 5   UnitPrice    541909 non-null  float64\n",
      " 6   CustomerID   406829 non-null  float64\n",
      " 7   Country      541909 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 33.1+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "df_retail = pd.read_csv('online_retail.csv')\n",
    "\n",
    "# Display the first 5 rows to understand the data structure\n",
    "print(\"Initial Data Preview:\")\n",
    "print(df_retail.head())\n",
    "\n",
    "# Display data types and non-null counts\n",
    "print(\"\\nData Information:\")\n",
    "df_retail.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63498162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Data Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 397884 entries, 0 to 541908\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   InvoiceNo    397884 non-null  object        \n",
      " 1   StockCode    397884 non-null  object        \n",
      " 2   Description  397884 non-null  object        \n",
      " 3   Quantity     397884 non-null  int64         \n",
      " 4   InvoiceDate  397884 non-null  datetime64[ns]\n",
      " 5   UnitPrice    397884 non-null  float64       \n",
      " 6   CustomerID   397884 non-null  object        \n",
      " 7   Country      397884 non-null  object        \n",
      " 8   TotalPrice   397884 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(5)\n",
      "memory usage: 30.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing CustomerID\n",
    "df_retail.dropna(subset=['CustomerID'], inplace=True)\n",
    "df_retail['CustomerID'] = df_retail['CustomerID'].astype(str)\n",
    "\n",
    "# Remove rows with negative Quantity or UnitPrice\n",
    "df_retail = df_retail[df_retail['Quantity'] > 0]\n",
    "df_retail = df_retail[df_retail['UnitPrice'] > 0]\n",
    "\n",
    "# Convert InvoiceDate to datetime object\n",
    "df_retail['InvoiceDate'] = pd.to_datetime(df_retail['InvoiceDate'])\n",
    "\n",
    "# Recalculate total sales for each transaction line\n",
    "df_retail['TotalPrice'] = df_retail['Quantity'] * df_retail['UnitPrice']\n",
    "\n",
    "# Display the cleaned data information\n",
    "print(\"\\nCleaned Data Information:\")\n",
    "df_retail.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee15d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RFM Table:\n",
      "            Recency  Frequency  Monetary\n",
      "CustomerID                              \n",
      "12346.0         326          1  77183.60\n",
      "12347.0           2        182   4310.00\n",
      "12348.0          75         31   1797.24\n",
      "12349.0          19         73   1757.55\n",
      "12350.0         310         17    334.40\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Set a reference date for calculating Recency (the day after the last transaction)\n",
    "snapshot_date = df_retail['InvoiceDate'].max() + dt.timedelta(days=1)\n",
    "\n",
    "# Calculate RFM metrics\n",
    "rfm_df = df_retail.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceNo': 'count',\n",
    "    'TotalPrice': 'sum'\n",
    "})\n",
    "\n",
    "# Rename the columns for clarity\n",
    "rfm_df.rename(columns={\n",
    "    'InvoiceDate': 'Recency',\n",
    "    'InvoiceNo': 'Frequency',\n",
    "    'TotalPrice': 'Monetary'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the RFM table\n",
    "print(\"\\nRFM Table:\")\n",
    "print(rfm_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebdc7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime as dt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6495f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Exploratory Data Analysis...\n"
     ]
    }
   ],
   "source": [
    "# Assume df_retail and rfm_df are already loaded and prepared from previous steps\n",
    "\n",
    "# Placeholder for df_retail, assuming it's loaded from the previous step\n",
    "df_retail = pd.read_csv('online_retail.csv')\n",
    "df_retail.dropna(subset=['CustomerID'], inplace=True)\n",
    "df_retail['CustomerID'] = df_retail['CustomerID'].astype(str)\n",
    "df_retail = df_retail[df_retail['Quantity'] > 0]\n",
    "df_retail = df_retail[df_retail['UnitPrice'] > 0]\n",
    "df_retail['InvoiceDate'] = pd.to_datetime(df_retail['InvoiceDate'])\n",
    "df_retail['TotalPrice'] = df_retail['Quantity'] * df_retail['UnitPrice']\n",
    "\n",
    "# Placeholder for rfm_df, assuming it's loaded from the previous step\n",
    "snapshot_date = df_retail['InvoiceDate'].max() + dt.timedelta(days=1)\n",
    "rfm_df = df_retail.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceNo': 'count',\n",
    "    'TotalPrice': 'sum'\n",
    "})\n",
    "rfm_df.rename(columns={\n",
    "    'InvoiceDate': 'Recency',\n",
    "    'InvoiceNo': 'Frequency',\n",
    "    'TotalPrice': 'Monetary'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Starting Exploratory Data Analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9237cfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Countries by Transaction Volume:\n",
      "Country\n",
      "United Kingdom    16646\n",
      "Germany             457\n",
      "France              389\n",
      "EIRE                260\n",
      "Belgium              98\n",
      "Netherlands          94\n",
      "Spain                90\n",
      "Portugal             57\n",
      "Australia            57\n",
      "Switzerland          51\n",
      "Name: InvoiceNo, dtype: int64\n",
      "\n",
      "Top 10 Selling Products by Quantity:\n",
      "Description\n",
      "PAPER CRAFT , LITTLE BIRDIE           80995\n",
      "MEDIUM CERAMIC TOP STORAGE JAR        77916\n",
      "WORLD WAR 2 GLIDERS ASSTD DESIGNS     54415\n",
      "JUMBO BAG RED RETROSPOT               46181\n",
      "WHITE HANGING HEART T-LIGHT HOLDER    36725\n",
      "ASSORTED COLOUR BIRD ORNAMENT         35362\n",
      "PACK OF 72 RETROSPOT CAKE CASES       33693\n",
      "POPCORN HOLDER                        30931\n",
      "RABBIT NIGHT LIGHT                    27202\n",
      "MINI PAINT SET VINTAGE                26076\n",
      "Name: Quantity, dtype: int64\n",
      "\n",
      "Purchase Trends Over Time:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91700\\AppData\\Local\\Temp\\ipykernel_7696\\536648351.py:34: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_sales = df_retail.set_index('InvoiceDate').resample('M')['TotalPrice'].sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monetary Distribution Analysis:\n",
      "\n",
      "RFM Distribution Analysis:\n",
      "\n",
      "Generating Elbow Curve for K-Means Clustering...\n",
      "\n",
      "Preparing for Customer Cluster Profiling...\n",
      "Customer Cluster Profiles:\n",
      "            Recency    Frequency       Monetary\n",
      "Cluster                                        \n",
      "0         41.341133   104.582512    2091.817116\n",
      "1          2.000000  5807.000000   70925.287500\n",
      "2        247.308333    27.787963     637.318510\n",
      "3          7.666667   826.833333  190863.461667\n",
      "\n",
      "Building Product Similarity Matrix for Recommendation...\n",
      "Product similarity matrix saved to 'product_similarity_matrix.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Analyze transaction volume by country\n",
    "print(\"\\nTop 10 Countries by Transaction Volume:\")\n",
    "country_transactions = df_retail.groupby('Country')['InvoiceNo'].nunique().sort_values(ascending=False).head(10)\n",
    "print(country_transactions)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=country_transactions.index, y=country_transactions.values)\n",
    "plt.title('Top 10 Countries by Transaction Volume')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('country_transaction_volume.png')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# 2. Identify top-selling products\n",
    "print(\"\\nTop 10 Selling Products by Quantity:\")\n",
    "top_products = df_retail.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "print(top_products)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_products.values, y=top_products.index)\n",
    "plt.title('Top 10 Selling Products by Quantity')\n",
    "plt.xlabel('Total Quantity Sold')\n",
    "plt.ylabel('Product Description')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_selling_products.png')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# 3. Visualize purchase trends over time\n",
    "print(\"\\nPurchase Trends Over Time:\")\n",
    "monthly_sales = df_retail.set_index('InvoiceDate').resample('M')['TotalPrice'].sum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=monthly_sales.index, y=monthly_sales.values)\n",
    "plt.title('Monthly Sales Trend Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales ($)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('monthly_sales_trend.png')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# 4. Inspect monetary distribution per transaction and customer\n",
    "print(\"\\nMonetary Distribution Analysis:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_retail['TotalPrice'], bins=50, kde=True)\n",
    "plt.title('Distribution of Transaction Monetary Value')\n",
    "plt.xlabel('Total Price ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(rfm_df['Monetary'], bins=50, kde=True)\n",
    "plt.title('Distribution of Customer Monetary Value')\n",
    "plt.xlabel('Monetary Value ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('monetary_distributions.png')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# 5. RFM distributions\n",
    "print(\"\\nRFM Distribution Analysis:\")\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(rfm_df['Recency'], bins=50, kde=True)\n",
    "plt.title('Recency Distribution')\n",
    "plt.xlabel('Recency (Days)')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(rfm_df['Frequency'], bins=50, kde=True)\n",
    "plt.title('Frequency Distribution')\n",
    "plt.xlabel('Frequency (Total Purchases)')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(rfm_df['Monetary'], bins=50, kde=True)\n",
    "plt.title('Monetary Distribution')\n",
    "plt.xlabel('Monetary Value ($)')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rfm_distributions.png')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# 6. Elbow curve for cluster selection\n",
    "print(\"\\nGenerating Elbow Curve for K-Means Clustering...\")\n",
    "rfm_scaled = rfm_df[['Recency', 'Frequency', 'Monetary']]\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_scaled)\n",
    "\n",
    "wcss = []\n",
    "k_values = range(1, 11)\n",
    "for i in k_values:\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)\n",
    "    kmeans.fit(rfm_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, wcss, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('elbow_curve.png')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# 7. Customer cluster profiles\n",
    "print(\"\\nPreparing for Customer Cluster Profiling...\")\n",
    "# This step requires choosing 'k' from the elbow curve.\n",
    "# For demonstration, let's assume k=4.\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42, n_init=10)\n",
    "rfm_df['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "cluster_profiles = rfm_df.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean()\n",
    "print(\"Customer Cluster Profiles:\")\n",
    "print(cluster_profiles)\n",
    "\n",
    "# Visualization of cluster profiles\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x=cluster_profiles.index, y=cluster_profiles['Recency'])\n",
    "plt.title('Recency by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean Recency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x=cluster_profiles.index, y=cluster_profiles['Frequency'])\n",
    "plt.title('Frequency by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x=cluster_profiles.index, y=cluster_profiles['Monetary'])\n",
    "plt.title('Monetary by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean Monetary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('customer_cluster_profiles.png')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# 8. Product recommendation heatmap / similarity matrix\n",
    "print(\"\\nBuilding Product Similarity Matrix for Recommendation...\")\n",
    "\n",
    "# Create a user-item matrix\n",
    "user_item_matrix = df_retail.pivot_table(index='CustomerID', columns='Description', values='Quantity').fillna(0)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "product_similarity_matrix = cosine_similarity(user_item_matrix.T)\n",
    "product_similarity_df = pd.DataFrame(product_similarity_matrix, index=user_item_matrix.columns, columns=user_item_matrix.columns)\n",
    "\n",
    "# Save the similarity matrix to a CSV file\n",
    "product_similarity_df.to_csv('product_similarity_matrix.csv')\n",
    "print(\"Product similarity matrix saved to 'product_similarity_matrix.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42749877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Step 4: Clustering Methodology...\n",
      "\n",
      "RFM values have been calculated in the previous step. Displaying head:\n",
      "            Recency  Frequency  Monetary\n",
      "CustomerID                              \n",
      "12346.0         326          1  77183.60\n",
      "12347.0           2        182   4310.00\n",
      "12348.0          75         31   1797.24\n",
      "12349.0          19         73   1757.55\n",
      "12350.0         310         17    334.40\n",
      "\n",
      "Standardizing RFM values...\n",
      "Standardized RFM values head:\n",
      "            Recency_Scaled  Frequency_Scaled  Monetary_Scaled\n",
      "CustomerID                                                   \n",
      "12346.0           2.334574         -0.396578         8.358668\n",
      "12347.0          -0.905340          0.394649         0.250966\n",
      "12348.0          -0.175360         -0.265435        -0.028596\n",
      "12349.0          -0.735345         -0.081836        -0.033012\n",
      "12350.0           2.174578         -0.326635        -0.191347\n",
      "\n",
      "Using Elbow Method and Silhouette Score to find optimal number of clusters (K)...\n",
      "\n",
      "Choosing optimal number of clusters as 4 based on the plots.\n",
      "\n",
      "K-Means clustering with K=4 completed.\n",
      "Size of each cluster:\n",
      "Cluster\n",
      "0    3248\n",
      "1       4\n",
      "2    1080\n",
      "3       6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cluster Profiles (Mean RFM values):\n",
      "            Recency    Frequency       Monetary\n",
      "Cluster                                        \n",
      "0         41.341133   104.582512    2091.817116\n",
      "1          2.000000  5807.000000   70925.287500\n",
      "2        247.308333    27.787963     637.318510\n",
      "3          7.666667   826.833333  190863.461667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import datetime as dt\n",
    "\n",
    "# Placeholder for df_retail, assuming it's loaded and cleaned from previous steps\n",
    "try:\n",
    "    df_retail = pd.read_csv('online_retail.csv')\n",
    "    df_retail.dropna(subset=['CustomerID'], inplace=True)\n",
    "    df_retail['CustomerID'] = df_retail['CustomerID'].astype(str)\n",
    "    df_retail = df_retail[df_retail['Quantity'] > 0]\n",
    "    df_retail = df_retail[df_retail['UnitPrice'] > 0]\n",
    "    df_retail['InvoiceDate'] = pd.to_datetime(df_retail['InvoiceDate'])\n",
    "    df_retail['TotalPrice'] = df_retail['Quantity'] * df_retail['UnitPrice']\n",
    "\n",
    "    # Placeholder for rfm_df, assuming it's loaded from previous steps\n",
    "    snapshot_date = df_retail['InvoiceDate'].max() + dt.timedelta(days=1)\n",
    "    rfm_df = df_retail.groupby('CustomerID').agg({\n",
    "        'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "        'InvoiceNo': 'count',\n",
    "        'TotalPrice': 'sum'\n",
    "    })\n",
    "    rfm_df.rename(columns={\n",
    "        'InvoiceDate': 'Recency',\n",
    "        'InvoiceNo': 'Frequency',\n",
    "        'TotalPrice': 'Monetary'\n",
    "    }, inplace=True)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'online_retail.csv' not found. Please make sure the file is in the same directory.\")\n",
    "    # Exit or handle the error gracefully\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "print(\"Starting Step 4: Clustering Methodology...\")\n",
    "\n",
    "# 1. Feature Engineering (RFM values already calculated in the previous step)\n",
    "print(\"\\nRFM values have been calculated in the previous step. Displaying head:\")\n",
    "print(rfm_df.head())\n",
    "\n",
    "# 2. Standardize/Normalize the RFM values\n",
    "print(\"\\nStandardizing RFM values...\")\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_df[['Recency', 'Frequency', 'Monetary']])\n",
    "rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=['Recency_Scaled', 'Frequency_Scaled', 'Monetary_Scaled'], index=rfm_df.index)\n",
    "\n",
    "print(\"Standardized RFM values head:\")\n",
    "print(rfm_scaled_df.head())\n",
    "\n",
    "# 3. & 4. Choose K-Means, and use Elbow Method & Silhouette Score\n",
    "# We will evaluate K from 2 to 10 for both methods\n",
    "print(\"\\nUsing Elbow Method and Silhouette Score to find optimal number of clusters (K)...\")\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "k_values = range(2, 11)\n",
    "\n",
    "for i in k_values:\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)\n",
    "    kmeans.fit(rfm_scaled_df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(rfm_scaled_df, kmeans.labels_))\n",
    "\n",
    "# Plotting the Elbow Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, wcss, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plotting the Silhouette Score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, silhouette_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_evaluation.png')\n",
    "plt.clf()\n",
    "\n",
    "# 5. Run Clustering\n",
    "# Based on the elbow curve and silhouette score plots, we will choose a value for K.\n",
    "# Both methods often suggest K=4 as a reasonable trade-off.\n",
    "optimal_k = 4\n",
    "print(f\"\\nChoosing optimal number of clusters as {optimal_k} based on the plots.\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42, n_init=10)\n",
    "rfm_df['Cluster'] = kmeans.fit_predict(rfm_scaled_df)\n",
    "\n",
    "print(f\"\\nK-Means clustering with K={optimal_k} completed.\")\n",
    "\n",
    "# Display the size of each cluster\n",
    "print(\"Size of each cluster:\")\n",
    "print(rfm_df['Cluster'].value_counts().sort_index())\n",
    "\n",
    "# Display the mean RFM values for each cluster to create profiles\n",
    "cluster_profiles = rfm_df.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean()\n",
    "print(\"\\nCluster Profiles (Mean RFM values):\")\n",
    "print(cluster_profiles)\n",
    "\n",
    "# Visualize cluster profiles\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x=cluster_profiles.index, y=cluster_profiles['Recency'])\n",
    "plt.title('Mean Recency by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean Recency (Days)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x=cluster_profiles.index, y=cluster_profiles['Frequency'])\n",
    "plt.title('Mean Frequency by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean Frequency (Purchases)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x=cluster_profiles.index, y=cluster_profiles['Monetary'])\n",
    "plt.title('Mean Monetary by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean Monetary ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_cluster_profiles.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec947117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KMeans model and scaler saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# ... (rest of your previous code) ...\n",
    "\n",
    "# 6. Visualize the clusters using a scatter plot or 3D plot of RFM scores.\n",
    "# (Code to be added later)\n",
    "\n",
    "# 7. Save the best performing model for streamlit usage\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(kmeans, 'kmeans_model.pkl')\n",
    "print(\"\\nKMeans model and scaler saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4be2e825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 00:52:32.897 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:32.898 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.338 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Shopper_Spectrum_Project\\venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-08-04 00:52:33.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.340 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.340 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.342 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.344 Session state does not function when running a script without `streamlit run`\n",
      "2025-08-04 00:52:33.344 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.344 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.345 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.345 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.346 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.346 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.348 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.349 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.349 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.350 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.350 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.350 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.353 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.353 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.354 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.354 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.355 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-04 00:52:33.355 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# =================================================================================\n",
    "# 1. Load Pre-trained Models and Data\n",
    "# =================================================================================\n",
    "\n",
    "try:\n",
    "    # Load the trained K-Means model and the scaler\n",
    "    kmeans = joblib.load('kmeans_model.pkl')\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "    # Load the product similarity matrix from the previous step\n",
    "    similarity_df = pd.read_csv('product_similarity_matrix.csv', index_col=0)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    st.error(\"Error: Model or data files not found. Please ensure 'kmeans_model.pkl', 'scaler.pkl', and 'product_similarity_matrix.csv' are in the same directory.\")\n",
    "    st.stop()\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# 2. Helper Functions\n",
    "# =================================================================================\n",
    "\n",
    "# Create a mapping from cluster number to a descriptive label\n",
    "cluster_labels = {\n",
    "    0: \"High-Value Shopper\", # These labels are based on the cluster profiling\n",
    "    1: \"Big Spender\",\n",
    "    2: \"At-Risk/Occasional\",\n",
    "    3: \"Regular/Promising\"\n",
    "}\n",
    "\n",
    "# Function to get product recommendations\n",
    "def recommend_products(product_name, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Finds and returns the top N similar products.\n",
    "    \"\"\"\n",
    "    # Check if the product exists in our similarity matrix\n",
    "    if product_name not in similarity_df.index:\n",
    "        return \"Product not found. Please try a different product name.\"\n",
    "\n",
    "    # Get the similarity scores for the given product\n",
    "    product_scores = similarity_df[product_name]\n",
    "\n",
    "    # Sort the scores and get the top N recommendations (excluding the product itself)\n",
    "    similar_products = product_scores.sort_values(ascending=False)\n",
    "    recommended = similar_products.index[1:n_recommendations+1].tolist()\n",
    "    return recommended\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# 3. Streamlit UI and App Logic\n",
    "# =================================================================================\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Shopper Spectrum Analytics\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Sidebar navigation\n",
    "st.sidebar.title(\"Navigation\")\n",
    "page = st.sidebar.radio(\"Go to\", [\"Product Recommender\", \"Customer Segmentation\"])\n",
    "\n",
    "if page == \"Product Recommender\":\n",
    "    st.title(\"ðŸ›’ Product Recommender\")\n",
    "    st.markdown(\"Enter a product name to get 5 similar product recommendations.\")\n",
    "    st.image(\"https://raw.githubusercontent.com/streamlit/streamlit/develop/docs/images/streamlit-logo-primary-colormark-light.png\", width=200) # Placeholder image\n",
    "    \n",
    "    # Get user input for product name\n",
    "    product_name_input = st.text_input(\"Enter Product Name (e.g., JUMBO BAG RED RETROSPOT)\", \"\")\n",
    "    \n",
    "    if st.button(\"Get Recommendations\"):\n",
    "        if product_name_input:\n",
    "            recommendations = recommend_products(product_name_input)\n",
    "            \n",
    "            if isinstance(recommendations, str):\n",
    "                st.warning(recommendations)\n",
    "            else:\n",
    "                st.subheader(\"Recommended Products:\")\n",
    "                for i, product in enumerate(recommendations, 1):\n",
    "                    st.write(f\"{i}. {product}\")\n",
    "        else:\n",
    "            st.warning(\"Please enter a product name.\")\n",
    "\n",
    "elif page == \"Customer Segmentation\":\n",
    "    st.title(\"ðŸ‘¤ Customer Segmentation\")\n",
    "    st.markdown(\"Enter RFM values to predict a customer's segment label.\")\n",
    "    \n",
    "    st.image(\"https://raw.githubusercontent.com/streamlit/streamlit/develop/docs/images/streamlit-logo-primary-colormark-light.png\", width=200) # Placeholder image\n",
    "    \n",
    "    # Get user input for RFM values\n",
    "    recency = st.number_input(\"Recency (days since last purchase)\", min_value=0, max_value=365, value=100)\n",
    "    frequency = st.number_input(\"Frequency (number of purchases)\", min_value=1, max_value=5000, value=50)\n",
    "    monetary = st.number_input(\"Monetary (total spend)\", min_value=0.0, max_value=200000.0, value=500.0, format=\"%.2f\")\n",
    "\n",
    "    if st.button(\"Predict Segment\"):\n",
    "        # Create a DataFrame for the user's input\n",
    "        input_data = pd.DataFrame([[recency, frequency, monetary]], columns=['Recency', 'Frequency', 'Monetary'])\n",
    "        \n",
    "        # Standardize the input data using the same scaler from training\n",
    "        scaled_input = scaler.transform(input_data)\n",
    "        \n",
    "        # Predict the cluster\n",
    "        predicted_cluster = kmeans.predict(scaled_input)[0]\n",
    "        \n",
    "        # Get the descriptive label from our mapping\n",
    "        predicted_label = cluster_labels.get(predicted_cluster, \"Unknown\")\n",
    "        \n",
    "        st.subheader(\"Prediction Result:\")\n",
    "        st.success(f\"This customer belongs to the **{predicted_label}** segment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
